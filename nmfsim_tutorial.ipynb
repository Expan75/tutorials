{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/bin/python3\n",
      "3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04) \n",
      "[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n",
      "sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SETUP \"\"\"\n",
    "# NOTE: this notebook requires Python3 (3.6.1) on the target machine.\n",
    "# To install Python3 and the additonal nessecary requirements (none for now),\n",
    "# please view: https://realpython.com/installing-python/\n",
    "# note that you need to specify the version (3.6.1) at install!\n",
    "\n",
    "# Getting pip and jupyter \n",
    "# $ python3 -m pip install --upgrade pip\n",
    "# $Â python3 -m pip install jupyter\n",
    "\n",
    "# Check correct python version (should say 3.6.1 at some point)\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PIP INSTALLS NECCESARY \"\"\"\n",
    "# Get scikit-learn and it's required packages (ML package + numerical computing)\n",
    "# $ pip3 install scikit-learn \n",
    "# Get pandas for easy wrangling and data management\n",
    "# $ pip3 install pandas=0.24.2\n",
    "\"\"\" Package imports \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF\n",
    "print(\"Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count:  11314\n"
     ]
    }
   ],
   "source": [
    "# First, let's load in the data and utilise some predefined cleaning tools, removing tags etc.\n",
    "raw = fetch_20newsgroups(shuffle=True, random_state=42, remove=['headers','footer','quotes'])\n",
    "data = raw.data \n",
    "print(\"Document count: \", len(data)) # how many documents do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Sample: 1...\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Printing Sample: 2...\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "Printing Sample: 3...\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try to only work with a subset (gotta go fast)\n",
    "\n",
    "n_samples = 2500 # how many documents we will process\n",
    "\n",
    "subsetRaw = data[:n_samples]\n",
    "\n",
    "# let's print out the first 3 samples! \n",
    "sub_samples = data[:3]\n",
    "for counter, sample in enumerate(sub_samples, 1):\n",
    "    print('Printing Sample: %d' %counter + '...')\n",
    "    print(\"\")\n",
    "    print(sample)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims of W: (2500, 12)\n",
      "Dims of H: (12, 1000)\n",
      "Topic #0: max 145 04 tm 14 45 au 75 34 mr\n",
      "Topic #1: available edu motif export version widget ftp based com pub\n",
      "Topic #2: people said don know didn went just armenians say like\n",
      "Topic #3: output file entry program stream null return line int open\n",
      "Topic #4: internet anonymous privacy email information use mail address file users\n",
      "Topic #5: turkish jews turkey nazis jewish war book history nazi university\n",
      "Topic #6: jesus matthew people said david king lord israel course does\n",
      "Topic #7: mv 17 27 sp ah 24 mw 145 mt 37\n",
      "Topic #8: edu os com comp cs ca yes john org ibm\n",
      "Topic #9: tape sys use windows scsi drive driver drivers disk problem\n",
      "Topic #10: 00 dos 15 20 good 50 25 excellent missing 10\n",
      "Topic #11: version machines type contact comments edu ftp pc anonymous available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Great! But you can't do math on raw words. How do we turn this stuff into math:able numbers? \n",
    "With NLP* ofc!\n",
    "\n",
    "*natural language processing\n",
    "\n",
    "The two most common starting points for processing text documents into numbers are:\n",
    " \n",
    "        1: Term Frequency [TF] (a.k.a. Bag-of-words model)\n",
    "        \n",
    "        2: TF-IDF (term frequency / inverse document frequency) # See https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "For the first part of this tutorial, we're gonna use Term Frequency.\n",
    "\"\"\"\n",
    "# Load our sample of the full data set. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "\n",
    "# Vectorizer + NMF params\n",
    "\n",
    "n_features = 1000 # how many raw \"features\" or data points per text we want to use\n",
    "n_components = 12 # how many groups (categories, in this case most ressembling \"topics\")\n",
    "n_top_words = 10 # the n most \"important\" words for each topic\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df = 0.95,\n",
    "    min_df = 10,\n",
    "    max_features = n_features,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(subsetRaw)\n",
    "tf.shape # Check dimensions of 2dim-array => Should output: (2500, 1000) => (n_rows, n_columns)\n",
    "\n",
    "# Let's set up NMF!\n",
    "# For a quick reminder: https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/\n",
    "\n",
    "# Setup /w params\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=42,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5\n",
    ")\n",
    "\n",
    "# Apply model to our data and get 2 resulting matrices (W, H)\n",
    "nmf.fit(tf)\n",
    "# Get matrices\n",
    "W = nmf.transform(tf)\n",
    "H = nmf.components_\n",
    "\n",
    "print(\"Dims of W: %s\" %str(W.shape))\n",
    "print(\"Dims of H: %s\" %str(H.shape))\n",
    "\n",
    "# Let's define a function to print the n_top_words in every category\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "# Let's print the top words behind every category/topic (i.e. the highest weighted words behind each topic)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Dims of Euc Matrix: \n",
      "(2500, 2500)\n",
      "Check Dims of Cos Matrix: \n",
      "(2500, 2500)\n",
      "Matrix symmetric: True\n",
      "Matrix symmetric: True\n",
      "['doc_698', 'doc_235', 'doc_708', 'doc_58', 'doc_2454', 'doc_1065', 'doc_493', 'doc_1841', 'doc_2302']\n",
      "['doc_1704', 'doc_58', 'doc_1116', 'doc_698', 'doc_708', 'doc_235', 'doc_2140', 'doc_1308', 'doc_2334']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "Okay, a little weird but fine. See instructor for explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Now we have condensed feature representations for every article we can start comparing them. For comparison,\n",
    "# we're going to employ Cos.Sim. and Euclidean Distance. We will also see the effect on our feature extraction!\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "# Via Euclidean on features\n",
    "euclideanDistanceMatrix = euclidean_distances(tf)\n",
    "print(\"Check Dims of Euc Matrix: \")\n",
    "print(euclideanDistanceMatrix.shape)\n",
    "\n",
    "# Via Cosim on raw features\n",
    "cosDistanceMatrix = cosine_distances(tf)\n",
    "print(\"Check Dims of Cos Matrix: \")\n",
    "print(cosDistanceMatrix.shape)\n",
    "\n",
    "# What should the dimensions be? How many samples did we use?\n",
    "# Let's check that we actually get a symmetric matrix for both:\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "for matrix in [euclideanDistanceMatrix, cosDistanceMatrix]:\n",
    "    print(\"Matrix symmetric: %s\" %check_symmetric(matrix))\n",
    "\n",
    "# Once we have this, let's query a document at random and find the 3 most similar ones.\n",
    "\n",
    "queryIndex = 255 # just any ol' postive integer (watch out for out of index, though!)\n",
    "# let's check what we got!\n",
    "subsetRaw[255]\n",
    "\n",
    "# Great! Let's pair this text up with a similar one and see if cos/euc yield different results\n",
    "\n",
    "# Before we get to the sorting step, we're going to utilise Pandas' dataframes to better keep track of our indexes; \n",
    "# this will ensure that we do not accidently mix up elements. \n",
    "# Btw, indexing starts from 0 in Python. Don't let matlab-people ruin your day.\n",
    "\n",
    "# Generate some more read:able names.\n",
    "featureNames = []\n",
    "for i in range(0,n_samples):\n",
    "    featureNames.append('doc_%d' %i)\n",
    "# print(featureNames)\n",
    "\n",
    "# Create Dataframes for each matrix:\n",
    "eucDf = pd.DataFrame(euclideanDistanceMatrix, columns=featureNames, index=featureNames)\n",
    "cosDf = pd.DataFrame(cosDistanceMatrix, columns=featureNames, index=featureNames)\n",
    "\n",
    "# Want to check what's cooking?\n",
    "# print(eucDf.head(3))\n",
    "# print(cosDf.head(3))\n",
    "# Notice something along the diagonal? Why is that?\n",
    "\n",
    "def getMostSimilarTextIndexes(index, df, n):\n",
    "    \"\"\" Retrieves the n most similar items' indexes and returns a list of indexes in order of similarity \"\"\"\n",
    "    listOfIndexes = []\n",
    "    doc = 'doc_%d' %index\n",
    "    n_most_similar_docs = df.nsmallest(n, doc, keep='all').index.tolist()\n",
    "    return n_most_similar_docs[1:]\n",
    "\n",
    "# Let's Compare and contrast!\n",
    "print(getMostSimilarTextIndexes(queryIndex, eucDf, 10))\n",
    "print(getMostSimilarTextIndexes(queryIndex, cosDf, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- COMPARING EUC -----------------------------------\n",
      "Printing the text we want to find friends to: \n",
      "\n",
      "No. When the program is run, it loads 4 configuration files; autoexec.bat,\n",
      "config.sys, win.ini, and system.ini. There is no Open entry on the File\n",
      "menu. You can only edit these four files. If you need to edit some other\n",
      "program's .ini file, use Notepad or some other ASCII editor.\n",
      "\n",
      "I wonder whether Microsoft intended for sysedit to be used, or if it was\n",
      "just a holdover from the testing period and they forgot to take it out. The\n",
      "reason I think this is because there is absolutely no mention in the manuals\n",
      "about this program, and there is no online help for it (just an About entry\n",
      "under the File menu). The program looks like something that was intended for\n",
      "internal use only. It's kind of a shame, though. It would have made a nice\n",
      "multi-file replacement for Notepad.\n",
      "\n",
      "Daniel Silevitch                           dmsilev@athena.mit.edu\n",
      "Massachusetts Institute of Technology\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 1 Most Similar\n",
      "There is a program called Graphic Workshop you can FTP from\n",
      "wuarchive.  The file is in the msdos/graphics directory and\n",
      "is called \"grfwk61t.zip.\"  This program should od everthing\n",
      "you need.\n",
      "\n",
      "-- \n",
      "\n",
      "TMC\n",
      "(tmc@spartan.ac.BrockU.ca)\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 2 Most Similar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Try setting up another HPIII printer but when choosing what port to connect it\n",
      "to choose FILE instead of like :LPT1.  This will prompt you for a file name\n",
      "everytime you print with that \"HPIII on FILE\" printer. Good Luck.\n",
      "\n",
      "\n",
      "Brian Servis\n",
      "===========================================================================\n",
      "===================================|| actual quotes from insurance claims||\n",
      "===========================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 3 Most Similar\n",
      "\n",
      "-- \n",
      "Hi netland,\n",
      "\n",
      "I thought that I once read about the existance of a virtual mwm like vtwm.\n",
      "On the usual ftp sites (gatakeeper.dec.com, export.lcs.mit.edu) I can't find\n",
      "any trace of this program. Could anybody give me a hint where to find this\n",
      "program or confirm/deny the existance of this program.\n",
      "\n",
      "Regards,\n",
      "\n",
      "  Stefan\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "----------------------------------- COMPARING COS -----------------------------------\n",
      "Printing the text we want to find friends to: \n",
      "\n",
      "No. When the program is run, it loads 4 configuration files; autoexec.bat,\n",
      "config.sys, win.ini, and system.ini. There is no Open entry on the File\n",
      "menu. You can only edit these four files. If you need to edit some other\n",
      "program's .ini file, use Notepad or some other ASCII editor.\n",
      "\n",
      "I wonder whether Microsoft intended for sysedit to be used, or if it was\n",
      "just a holdover from the testing period and they forgot to take it out. The\n",
      "reason I think this is because there is absolutely no mention in the manuals\n",
      "about this program, and there is no online help for it (just an About entry\n",
      "under the File menu). The program looks like something that was intended for\n",
      "internal use only. It's kind of a shame, though. It would have made a nice\n",
      "multi-file replacement for Notepad.\n",
      "\n",
      "Daniel Silevitch                           dmsilev@athena.mit.edu\n",
      "Massachusetts Institute of Technology\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 1 Most Similar\n",
      "There is a program called Graphic Workshop you can FTP from\n",
      "wuarchive.  The file is in the msdos/graphics directory and\n",
      "is called \"grfwk61t.zip.\"  This program should od everthing\n",
      "you need.\n",
      "\n",
      "-- \n",
      "\n",
      "TMC\n",
      "(tmc@spartan.ac.BrockU.ca)\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 2 Most Similar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Try setting up another HPIII printer but when choosing what port to connect it\n",
      "to choose FILE instead of like :LPT1.  This will prompt you for a file name\n",
      "everytime you print with that \"HPIII on FILE\" printer. Good Luck.\n",
      "\n",
      "\n",
      "Brian Servis\n",
      "===========================================================================\n",
      "===================================|| actual quotes from insurance claims||\n",
      "===========================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Printing number 3 Most Similar\n",
      "\n",
      "-- \n",
      "Hi netland,\n",
      "\n",
      "I thought that I once read about the existance of a virtual mwm like vtwm.\n",
      "On the usual ftp sites (gatakeeper.dec.com, export.lcs.mit.edu) I can't find\n",
      "any trace of this program. Could anybody give me a hint where to find this\n",
      "program or confirm/deny the existance of this program.\n",
      "\n",
      "Regards,\n",
      "\n",
      "  Stefan\n",
      "\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Notice how using Euclidean distance and cosine difference yielded similar but ultimately different results. y though?\n",
    "# Also notice the first result is the document we queried. \n",
    "# After all, the most similar thing to a thing, is the thing itself.\n",
    "\n",
    "\"\"\"\n",
    "Now with some actual results, let's check if we agree with the computer's decision. Let's get the texts and compare.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "eucRes = getMostSimilarTextIndexes(queryIndex, eucDf, 3)\n",
    "cosRes = getMostSimilarTextIndexes(queryIndex, cosDf, 3)\n",
    "\n",
    "def comparePrint(res, sourceDocIndex):\n",
    "    print(\"Printing the text we want to find friends to: \")\n",
    "    print(subsetRaw[sourceDocIndex])\n",
    "    print(\"\")\n",
    "    for counter, doc_id in enumerate(res, 1):\n",
    "        print('-'*50)\n",
    "        print(\"Printing number %d Most Similar\" % counter)\n",
    "        print(subsetRaw[int(doc_id[4:])])\n",
    "    return \"Done!\"\n",
    "\n",
    "print(\"----------------------------------- COMPARING EUC -----------------------------------\")\n",
    "comparePrint(eucRes, queryIndex)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"----------------------------------- COMPARING COS -----------------------------------\")\n",
    "comparePrint(eucRes, queryIndex)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been the tutorial!\n",
    "\n",
    "Please take some time and go through each part to make sure you understand the gist. Please do play around\n",
    "with parameters and see if you can achieve better result in the comparisons of text!\n",
    "\n",
    "Arguably, the most important step in the whole comparsion process is the preproccessing. You can try to play around\n",
    "with other vectorizers (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text).\n",
    "\n",
    "Additionally, you could try to define your own preprocessing steps. Like we saw in our applied NMF, a lot of artifacts\n",
    "of 2-character sequences get thrown in the mix. Perhaps you could try to cut away 2-character 'words' before applying\n",
    "vectorization (HINT: Regular expressions!!!).\n",
    "\n",
    "If you want something really challenging, try using the vectorized text and build a classifier to predict the group.\n",
    "The dataset contains labels, but you'll need to load those in along with the rest! \n",
    "\n",
    "If you have any questions regarding the tutorial, please email:\n",
    "erik.hakansson96@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
